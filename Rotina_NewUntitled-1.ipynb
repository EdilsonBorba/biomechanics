{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play para incluir os inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cf8000e83c4c93b490108c889eb266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Name:'), Dropdown(description='Leg:', options=('Right', 'Left'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By Edilson Borba \n",
    "    # borba.edi@gmail.com\n",
    "        # If you need suport, send me an email. I'm happy to help\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import asyncio\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "# Function to calculate the dominant frequency of the signal\n",
    "def calculate_frequency(signal, Frequency):\n",
    "    n = len(signal)\n",
    "    yf = np.fft.fft(signal)\n",
    "    xf = np.fft.fftfreq(n, 1 / Frequency)\n",
    "    idx = np.argmax(np.abs(yf))\n",
    "    frequency = np.abs(xf[idx])\n",
    "    return frequency, xf, yf\n",
    "\n",
    "# Function to calculate the median frequency of the signal\n",
    "def calculate_median_frequency(yf, xf):\n",
    "    power_spectrum = np.abs(yf)**2\n",
    "    cumulative_power = np.cumsum(power_spectrum)\n",
    "    total_power = cumulative_power[-1]\n",
    "    median_idx = np.where(cumulative_power >= total_power / 2)[0][0]\n",
    "    median_frequency = np.abs(xf[median_idx])\n",
    "    return median_frequency\n",
    "\n",
    "Cropping_Duration = 30  # Define the center cropping duration in seconds - For example, crop to a 30-second duration\n",
    "\n",
    "Window_Duration = 5 # Time in seconds for each analysis window\n",
    "\n",
    "# Additional Parameters\n",
    "Min_Distance = 20 # Minimum distance between points \n",
    "Positive_Height = 0.2 # Minimum cut-off line for marking positive points \n",
    "Negative_Height = 0.2 # Minimum cut-off line for marking negative points\n",
    "\n",
    "# Applied Filter (Savitzky-Golay - Schafer, R. W. (2011). What is a Savitzky-Golay filter?. IEEE Signal processing magazine, 28(4), 111-117)\n",
    "Savi_Length = 7  # The length of the Savitzky-Golay filter window (Always odd numbers and greater than polyorder)\n",
    "Savi_Order = 3 # The order of the Savitzky-Golay filter\n",
    "\n",
    "# Function to create a file chooser widget\n",
    "def create_file_chooser(description):\n",
    "    label = widgets.Label(description)\n",
    "    chooser = FileChooser()\n",
    "    return widgets.VBox([label, chooser]), chooser\n",
    "\n",
    "# Create input widgets\n",
    "Name = widgets.Text(description='Name:')\n",
    "Leg = widgets.Dropdown(options=['Right', 'Left'], description='Leg:')\n",
    "Data_Used = widgets.Dropdown(options=['Gyro', 'Accel'], description='Data Used:')\n",
    "file_input_container, file_input_chooser = create_file_chooser('Select Input File')\n",
    "file_output_container, file_output_chooser = create_file_chooser('Select Output Path')\n",
    "\n",
    "global Name\n",
    "global Leg\n",
    "global Data_Used\n",
    "\n",
    "def on_button_click(button):\n",
    "    global File_Input, File_Output  # Declare File_Input as global if it's used elsewhere outside this function\n",
    "    selected_input_file = file_input_chooser.selected\n",
    "    selected_output_file = file_output_chooser.selected\n",
    "    if selected_input_file and selected_output_file:\n",
    "        File_Input = selected_input_file  # Set File_Input as the selected input file\n",
    "        File_Output = selected_output_file\n",
    "        print(f\"Name: {Name.value}\")\n",
    "        print(f\"Leg: {Leg.value}\")\n",
    "        print(f\"Data Used: {Data_Used.value}\")\n",
    "        print(f\"Selected Input File: {selected_input_file}\")\n",
    "        print(f\"Selected Output Path: {selected_output_file}\")\n",
    "\n",
    "        \n",
    "        # Reading the .csv file using pandas\n",
    "        File_Output_Name = f\"{Name.value}.xlsx\"\n",
    "        File_Path_Full = os.path.join(selected_input_file)\n",
    "        df = pd.read_csv(File_Path_Full)\n",
    "\n",
    "        # Saving the Data Frame as an Excel file\n",
    "        global File_Output_Path_Full\n",
    "        File_Output_Path_Full = os.path.join(selected_output_file, File_Output_Name)\n",
    "        df.to_excel(File_Output_Path_Full, index=False)\n",
    "        print(f\"Excel file saved at: {File_Output_Path_Full}\")\n",
    "        \n",
    "        # Reading the saved Excel file\n",
    "        global data\n",
    "        data = pd.read_excel(File_Output_Path_Full)\n",
    "        \n",
    "        # Continue with the rest of the code after confirmation\n",
    "        print(\"Continuing with the rest of the code...\")\n",
    "    else:\n",
    "        print(\"Please select both input and output files\")\n",
    "\n",
    "# Create a button widget\n",
    "display_button = widgets.Button(description=\"Confirm and Continue\")\n",
    "display_button.on_click(on_button_click)\n",
    "\n",
    "# Group widgets into a single container\n",
    "container = widgets.VBox([Name, Leg, Data_Used, file_input_container, file_output_container, display_button])\n",
    "\n",
    "# Display the container\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play para realizar a análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy array\n",
    "Data = np.array(data)\n",
    "\n",
    "# Extract columns\n",
    "Time = Data[:,0]\n",
    "Accelerometer = Data[:,12]\n",
    "Gyroscope = Data[:,14]\n",
    "\n",
    "# Convert the inverted Accelerometer data from g to m/s^2\n",
    "Accelerometer = Accelerometer * -9.81\n",
    "offset_value = Accelerometer[0]\n",
    "Accelerometer = Accelerometer - offset_value\n",
    "\n",
    "# List to store processed times\n",
    "processed_times = []\n",
    "\n",
    "# Iterate over each element in the Time list\n",
    "for t in Time:\n",
    "    # Split the string using \"T\" as separator and take the second part (after \"T\")\n",
    "    processed_time = t.split(\"T\")[1]\n",
    "    # Remove the time zone offset (-03:00)\n",
    "    processed_time = processed_time.split(\"-\")[0]\n",
    "    # Add the processed time to the list\n",
    "    processed_times.append(processed_time)\n",
    "\n",
    "# Convert the processed times into datetime objects\n",
    "datetime_times = [datetime.strptime(time, \"%H:%M:%S.%f\") for time in processed_times]\n",
    "\n",
    "# Calculate the time differences between each consecutive pair of times\n",
    "differences = [datetime_times[i+1] - datetime_times[i] for i in range(len(datetime_times)-1)]\n",
    "\n",
    "# Calculate the mean of the time differences in seconds\n",
    "total_difference = sum(differences, timedelta())\n",
    "mean_difference = total_difference.total_seconds() / len(differences)\n",
    "\n",
    "# Calculate frequency based on mean time difference\n",
    "Time_Diff = mean_difference\n",
    "Frequency = 1 / Time_Diff\n",
    "\n",
    "# Apply the Savitzky-Golay filter to the accelerometer data\n",
    "Accelerometer_filtered = savgol_filter(Accelerometer, Savi_Length, Savi_Order)\n",
    "\n",
    "# Apply the Savitzky-Golay filter to the gyroscope data\n",
    "Gyro_filtered = savgol_filter(Gyroscope, Savi_Length, Savi_Order)\n",
    "\n",
    "if Data_Used == 'Accel':\n",
    "    Filtered_Data = Accelerometer_filtered\n",
    "else:\n",
    "    Filtered_Data = Gyro_filtered\n",
    "    \n",
    "if Data_Used == 'Accel':\n",
    "    Positive_Height *= 10\n",
    "    Negative_Height *= 10\n",
    "else:\n",
    "    pass\n",
    "\n",
    "peaks, _ = find_peaks(Filtered_Data, height=Positive_Height, distance=Min_Distance)  # Limit the positive threshold\n",
    "neg_peaks, _ = find_peaks(-Filtered_Data, height=Negative_Height, distance=Min_Distance) # Limit the negative threshold\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(Filtered_Data, label='Signal')\n",
    "plt.axhline(y=0, color='black', linewidth=0.6)\n",
    "plt.plot(peaks, Filtered_Data[peaks], 'gx', label='Peaks')\n",
    "plt.plot(neg_peaks, Filtered_Data[neg_peaks], 'rx', label='Troughs')\n",
    "\n",
    "zero_crossings = np.where(np.diff(np.sign(Filtered_Data)))[0]\n",
    "\n",
    "onset_times = []\n",
    "completion_times = []\n",
    "zero_after_peak_times = []\n",
    "ascent_times = []\n",
    "descent_times = []\n",
    "cycle_durations = []\n",
    "transition_times = []\n",
    "\n",
    "# Find peaks\n",
    "for peak in peaks:\n",
    "    # Find the onset time (last zero crossing before the peak)\n",
    "    zero_before = zero_crossings[zero_crossings < peak]\n",
    "    if zero_before.size > 0:\n",
    "        onset_time = zero_before[-1]\n",
    "        onset_times.append(onset_time)\n",
    "        plt.plot(onset_time, 0, 'o', color='green', label='Onset', markersize=5)\n",
    "\n",
    "        # Find the next trough after the peak\n",
    "        troughs_after_peak = neg_peaks[neg_peaks > peak]\n",
    "        if troughs_after_peak.size > 0:\n",
    "            trough = troughs_after_peak[0]\n",
    "            # Find the completion time (first zero crossing after the trough)\n",
    "            zero_after = zero_crossings[zero_crossings > trough]\n",
    "            if zero_after.size > 0:\n",
    "                completion_time = zero_after[0]\n",
    "                completion_times.append(completion_time)\n",
    "                plt.plot(completion_time, 0, 'o', color='red', label='Completion', markersize=5)\n",
    "\n",
    "    # Find the first zero crossing after the peak\n",
    "    zero_after_peak = zero_crossings[zero_crossings > peak]\n",
    "    if zero_after_peak.size > 0:\n",
    "        first_zero_after_peak = zero_after_peak[0]\n",
    "        zero_after_peak_times.append(first_zero_after_peak)\n",
    "        plt.plot(first_zero_after_peak, 0, 'o', color='black', label='Intermediate', markersize=5)\n",
    "\n",
    "for i, peak in enumerate(peaks):\n",
    "    plt.text(peak, Filtered_Data[peak] + (max(Filtered_Data) - min(Filtered_Data)) * 0.03, str(i + 1),\n",
    "            color='black', fontsize=9, ha='center')\n",
    "\n",
    "peak_count = len(peaks)\n",
    "print(f\"Number total of cycles: {peak_count}\")\n",
    "\n",
    "if onset_times:\n",
    "    first_onset_time = onset_times[0]\n",
    "    cutoff_index = first_onset_time + int(Cropping_Duration * Frequency)\n",
    "    Filtered_Data = Filtered_Data[first_onset_time:cutoff_index]\n",
    "\n",
    "peaks, _ = find_peaks(Filtered_Data, height=Positive_Height, distance=Min_Distance)  # Limit the positive threshold\n",
    "neg_peaks, _ = find_peaks(-Filtered_Data, height=Negative_Height, distance=Min_Distance) # Limit the negative threshold\n",
    "\n",
    "plt.figure(figsize=(13, 5))\n",
    "plt.plot(Filtered_Data, label='Signal')\n",
    "plt.axhline(y=0, color='black', linewidth=0.6)\n",
    "plt.plot(peaks, Filtered_Data[peaks], 'gx', label='Peaks')\n",
    "plt.plot(neg_peaks, Filtered_Data[neg_peaks], 'rx', label='Troughs')\n",
    "\n",
    "zero_crossings = np.where(np.diff(np.sign(Filtered_Data)))[0]\n",
    "\n",
    "onset_times = []\n",
    "completion_times = []\n",
    "zero_after_peak_times = []\n",
    "ascent_times = []\n",
    "descent_times = []\n",
    "cycle_durations = []\n",
    "transition_times = []\n",
    "\n",
    "# Find peaks\n",
    "for peak in peaks:\n",
    "    # Find the onset time (last zero crossing before the peak)\n",
    "    zero_before = zero_crossings[zero_crossings < peak]\n",
    "    if zero_before.size > 0:\n",
    "        onset_time = zero_before[-1]\n",
    "        onset_times.append(onset_time)\n",
    "        plt.plot(onset_time, 0, 'o', color='green', label='Onset', markersize=5)\n",
    "\n",
    "        # Find the next trough after the peak\n",
    "        troughs_after_peak = neg_peaks[neg_peaks > peak]\n",
    "        if troughs_after_peak.size > 0:\n",
    "            trough = troughs_after_peak[0]\n",
    "            # Find the completion time (first zero crossing after the trough)\n",
    "            zero_after = zero_crossings[zero_crossings > trough]\n",
    "            if zero_after.size > 0:\n",
    "                completion_time = zero_after[0]\n",
    "                completion_times.append(completion_time)\n",
    "                plt.plot(completion_time, 0, 'o', color='red', label='Completion', markersize=5)\n",
    "\n",
    "    # Find the first zero crossing after the peak\n",
    "    zero_after_peak = zero_crossings[zero_crossings > peak]\n",
    "    if zero_after_peak.size > 0:\n",
    "        first_zero_after_peak = zero_after_peak[0]\n",
    "        zero_after_peak_times.append(first_zero_after_peak)\n",
    "        plt.plot(first_zero_after_peak, 0, 'o', color='black', label='Intermediate', markersize=5)\n",
    "\n",
    "for i, peak in enumerate(peaks):\n",
    "    plt.text(peak, Filtered_Data[peak] + (max(Filtered_Data) - min(Filtered_Data)) * 0.03, str(i + 1),\n",
    "            color='black', fontsize=9, ha='center')\n",
    "\n",
    "# Remove duplicate legend entries\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), loc='upper left', bbox_to_anchor=(0.989, 1), frameon=False)\n",
    "\n",
    "# Calculate ascent and descent times\n",
    "for i in range(min(len(onset_times), len(zero_after_peak_times), len(completion_times))):\n",
    "    ascent_time = (zero_after_peak_times[i] - onset_times[i]) / Frequency\n",
    "    descent_time = (completion_times[i] - zero_after_peak_times[i]) / Frequency\n",
    "    ascent_times.append(ascent_time)\n",
    "    descent_times.append(descent_time)\n",
    "\n",
    "# Calculate the average ascent and descent times\n",
    "avg_ascent_time = sum(ascent_times) / len(ascent_times) if ascent_times else 0\n",
    "avg_descent_time = sum(descent_times) / len(descent_times) if descent_times else 0\n",
    "\n",
    "# Calculate the average time difference between each onset and the subsequent completion\n",
    "if len(onset_times) > 0 and len(completion_times) > 0:\n",
    "    time_differences = [completion_times[i] - onset_times[i] for i in\n",
    "                        range(min(len(onset_times), len(completion_times)))]\n",
    "\n",
    "    # Filter out any negative differences\n",
    "    time_differences = [td for td in time_differences if td > 0]\n",
    "    if time_differences:\n",
    "        average_time_difference = sum(time_differences) / len(time_differences)\n",
    "        average_time_difference_seconds = average_time_difference / Frequency\n",
    "        #print(f\"Average cycle duration: {average_time_difference_seconds:.2f}s\")\n",
    "        cycle_durations = [td / Frequency for td in time_differences]\n",
    "    else:\n",
    "        average_time_difference_seconds = 0\n",
    "\n",
    "# Calculate the transition times starting from the second cycle\n",
    "for i in range(1, len(completion_times)):\n",
    "    transition_time = (onset_times[i] - completion_times[i - 1]) / Frequency\n",
    "    transition_times.append(transition_time)\n",
    "\n",
    "# Calculate the average transition time\n",
    "avg_transition_time = sum(transition_times) / len(transition_times) if transition_times else 0\n",
    "\n",
    "# Calculate the standard deviation and CV for cycle durations\n",
    "avg_cycle_duration = average_time_difference_seconds\n",
    "std_cycle_duration = np.std(cycle_durations) if cycle_durations else 0\n",
    "cv_cycle_duration = std_cycle_duration / avg_cycle_duration * 100 if avg_cycle_duration else 0\n",
    "\n",
    "# Calculate the standard deviation for ascent, descent, and transition times\n",
    "std_ascent_time = np.std(ascent_times) if ascent_times else 0\n",
    "std_descent_time = np.std(descent_times) if descent_times else 0\n",
    "std_transition_time = np.std(transition_times) if transition_times else 0\n",
    "\n",
    "# Calculate the coefficient of variation (CV) for ascent, descent, and transition times\n",
    "cv_ascent_time = std_ascent_time / avg_ascent_time * 100 if avg_ascent_time else 0\n",
    "cv_descent_time = std_descent_time / avg_descent_time * 100 if avg_descent_time else 0\n",
    "cv_transition_time = std_transition_time / avg_transition_time * 100 if avg_transition_time else 0\n",
    "\n",
    "avg_peak_value = sum(Filtered_Data[peaks]) / len(peaks) if len(peaks) > 0 else 0\n",
    "avg_trough_value = sum(Filtered_Data[neg_peaks]) / len(neg_peaks) if len(neg_peaks) > 0 else 0\n",
    "peak_count = len(peaks)\n",
    "trough_count = len(neg_peaks)\n",
    "\n",
    "# Calculate the standard deviation and CV for peak and trough accelerations\n",
    "std_peak_value = np.std(Filtered_Data[peaks]) if len(peaks) > 0 else 0\n",
    "std_trough_value = np.std(Filtered_Data[neg_peaks]) if len(neg_peaks) > 0 else 0\n",
    "cv_peak_value = std_peak_value / avg_peak_value * 100 if avg_peak_value else 0\n",
    "cv_trough_value = std_trough_value / avg_trough_value * 100 if avg_trough_value else 0\n",
    "\n",
    "plt.title(f'\\nMean Values: Peak {avg_peak_value:.3f} | Trough: {avg_trough_value:.3f} | Cycle: {average_time_difference_seconds:.3f}s | Ascent: {avg_ascent_time:.3f}s | Descent: {avg_descent_time:.3f}s | Transition: {avg_transition_time:.3f}s')\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.show()\n",
    "print(f\"Number of cycles: {peak_count}\")\n",
    "\n",
    "# Determine the number of samples per window\n",
    "Samples_Window = int(Window_Duration * Frequency)\n",
    "\n",
    "# Calculate the number of windows\n",
    "Num_Windows = len(Filtered_Data) // Samples_Window\n",
    "\n",
    "# Lists to store median amplitude and frequency for each window\n",
    "median_amplitudes = []\n",
    "median_frequencies = []\n",
    "\n",
    "# Lists to store peak frequency and amplitude for each window\n",
    "peak_frequencies = []\n",
    "peak_amplitudes = []\n",
    "\n",
    "# Figure for subplots\n",
    "fig, axs = plt.subplots(Num_Windows, 2, figsize=(12, 2 * Num_Windows))\n",
    "\n",
    "# Fourier analysis in 5-second windows\n",
    "for i in range(Num_Windows):\n",
    "    start_idx = i * Samples_Window\n",
    "    end_idx = start_idx + Samples_Window\n",
    "    window_signal = Filtered_Data[start_idx:end_idx]\n",
    "    \n",
    "    # Calculate time for the current window\n",
    "    t = np.linspace(0, Window_Duration, Samples_Window, endpoint=False)\n",
    "    \n",
    "    # Calculate the dominant frequency of the signal\n",
    "    calculated_freq, xf, yf = calculate_frequency(window_signal, Frequency)\n",
    "    \n",
    "    # Find the index of the peak frequency\n",
    "    peak_idx = np.argmax(np.abs(yf))\n",
    "    peak_frequency = xf[peak_idx]\n",
    "    peak_amplitude = np.abs(yf)[peak_idx]\n",
    "    if peak_frequency < 0:\n",
    "        peak_frequency = peak_frequency *-1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Add peak frequency and amplitude to the lists\n",
    "    peak_frequencies.append(peak_frequency)\n",
    "    peak_amplitudes.append(peak_amplitude)\n",
    "    \n",
    "    # Calculate the median frequency of the signal\n",
    "    median_frequency = calculate_median_frequency(yf, xf)\n",
    "    median_frequencies.append(median_frequency)\n",
    "    \n",
    "    # Calculate the median amplitude of the signal\n",
    "    median_amplitude = np.median(window_signal)\n",
    "    median_amplitudes.append(median_amplitude)\n",
    "    \n",
    "    # Plot of the signal in the time domain\n",
    "    plt.subplots_adjust(hspace = 0.8, wspace = 0.2)\n",
    "    \n",
    "    axs[i, 0].plot(t, window_signal)\n",
    "    axs[i, 0].set_title(f'Signal in Time Domain - Window {i+1}')\n",
    "    axs[i, 0].set_xlabel('Time [s]')\n",
    "    axs[i, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot of the signal in the frequency domain\n",
    "    axs[i, 1].plot(xf, np.abs(yf))\n",
    "    axs[i, 1].axvline(peak_frequency, color='r', linestyle='--')  # Add a vertical line at the peak frequency\n",
    "    axs[i, 1].set_title(f'Signal in Frequency Domain - Window {i+1}')\n",
    "    axs[i, 1].set_xlabel('Frequency [Hz]')\n",
    "    axs[i, 1].set_ylabel('Magnitude')\n",
    "    axs[i, 1].set_xlim(0, Frequency / 2)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Window')\n",
    "ax1.set_ylabel('Peak Frequency (Hz)', color=color)\n",
    "ax1.plot(range(1, Num_Windows + 1), peak_frequencies, color=color, marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Peak Amplitude', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(range(1, Num_Windows + 1), peak_amplitudes, color=color, marker='o')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title('Peak Frequencies and Amplitudes per Window')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Defining the DataFrame with the data\n",
    "df_Export_Data = pd.DataFrame({\n",
    "    'Name': [Name.value],\n",
    "    'Leg': [Leg.value],\n",
    "    'Frequency': [round(Frequency, 3)],\n",
    "    'Delta Time': [round(Time_Diff, 3)],\n",
    "    'Used Data': [Data_Used.value],\n",
    "    'Number of Cycles': [peak_count],\n",
    "    'Transition Time': [round(avg_transition_time, 3)],\n",
    "    'Cycle Duration': [round(avg_cycle_duration, 3)],\n",
    "    'SD Cycle Duration': [round(std_cycle_duration, 3)],\n",
    "    'Ascent Time': [round(avg_ascent_time, 3)],\n",
    "    'SD Ascent Time': [round(std_ascent_time, 3)],\n",
    "    'Descent Time': [round(avg_descent_time, 3)],\n",
    "    'SD Descent Time': [round(std_descent_time, 3)],\n",
    "    'Peak': [round(avg_peak_value, 3)],\n",
    "    'SD Peak': [round(std_peak_value, 3)],\n",
    "    'Trough': [round(avg_trough_value, 3)],\n",
    "    'SD Trough': [round(std_trough_value, 3)],\n",
    "    **{f'Median Amplitude {i}': round(amp, 3) for i, amp in enumerate(median_amplitudes, 1)},\n",
    "    **{f'Median Frequency {i}': round(freq, 3) for i, freq in enumerate(median_frequencies, 1)},\n",
    "    **{f'Peak Frequency {i}': round(freq, 3) for i, freq in enumerate(peak_frequencies, 1)},\n",
    "    **{f'Peak Amplitude {i}': round(amp, 3) for i, amp in enumerate(peak_amplitudes, 1)}\n",
    "})\n",
    "\n",
    "file_name = f\"{Name.value}_{Leg.value}_{Data_Used.value}.xlsx\"\n",
    "file_output_path_full = os.path.join(File_Output, file_name)\n",
    "\n",
    "# Saving the DataFrame as an XLSX file\n",
    "df_Export_Data.to_excel(file_output_path_full, index=False)\n",
    "\n",
    "print(f'Exportado para {file_output_path_full}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
